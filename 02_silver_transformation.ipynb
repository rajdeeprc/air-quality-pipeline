{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbd78eda-886b-4ee0-ae69-e24aae48e200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83E\uDD48 Silver Layer — Transformation & Validation Notebook\n",
    "## Notebook: 02_silver_transformation\n",
    "\n",
    "**What this notebook does:**\n",
    "- Reads raw data from Bronze Delta table\n",
    "- Removes duplicates and null values\n",
    "- Validates pollutant readings (no negatives, no nulls)\n",
    "- Assigns AQI categories to PM2.5 readings\n",
    "- Writes clean, validated data to Silver Delta table\n",
    "\n",
    "**Run after:** `01_bronze_ingestion`\n",
    "**Run before:** `03_gold_aggregation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "878a1fc4-570a-48b9-847d-a6e660ddc023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration reloaded.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RELOAD CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "DATABASE_NAME = \"air_quality_db\"\n",
    "\n",
    "BRONZE_TABLE     = f\"{DATABASE_NAME}.bronze_raw_measurements\"\n",
    "SILVER_TABLE     = f\"{DATABASE_NAME}.silver_clean_measurements\"\n",
    "GOLD_TABLE_CITY  = f\"{DATABASE_NAME}.gold_city_rankings\"\n",
    "GOLD_TABLE_TREND = f\"{DATABASE_NAME}.gold_pollutant_trends\"\n",
    "GOLD_TABLE_AQI   = f\"{DATABASE_NAME}.gold_aqi_summary\"\n",
    "\n",
    "TARGET_POLLUTANTS = [\"pm25\", \"pm10\", \"no2\", \"o3\", \"co\", \"so2\"]\n",
    "\n",
    "AQI_CATEGORIES = {\n",
    "    \"Good\":                  (0.0,   12.0),\n",
    "    \"Moderate\":              (12.1,  35.4),\n",
    "    \"Unhealthy (Sensitive)\": (35.5,  55.4),\n",
    "    \"Unhealthy\":             (55.5,  150.4),\n",
    "    \"Very Unhealthy\":        (150.5, 250.4),\n",
    "    \"Hazardous\":             (250.5, 9999.0)\n",
    "}\n",
    "\n",
    "spark.sql(f\"USE {DATABASE_NAME}\")\n",
    "\n",
    "print(\"✅ Configuration reloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5220721-23ef-4c9f-bdc9-195c493c1c01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bronze records loaded : 7215\n\n\uD83D\uDCCA Schema:\nroot\n |-- city: string (nullable = true)\n |-- country: string (nullable = true)\n |-- pollutant: string (nullable = true)\n |-- value: double (nullable = true)\n |-- unit: string (nullable = true)\n |-- location_name: string (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- measured_at: timestamp (nullable = true)\n |-- ingested_at: timestamp (nullable = true)\n |-- source_url: string (nullable = true)\n |-- raw_json: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# READ FROM BRONZE TABLE\n",
    "# ============================================================\n",
    "\n",
    "bronze_df = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {BRONZE_TABLE}\n",
    "    WHERE city != 'TestCity'\n",
    "\"\"\")\n",
    "\n",
    "bronze_count = bronze_df.count()\n",
    "\n",
    "print(f\"✅ Bronze records loaded : {bronze_count}\")\n",
    "print(f\"\\n\uD83D\uDCCA Schema:\")\n",
    "bronze_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "151343e9-9707-41b8-8c53-bcce4c94114f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Deduplication complete.\n   Records before : 7215\n   Duplicates removed : 0\n   Records after  : 7215\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 1 — REMOVE DUPLICATES\n",
    "# A duplicate is same city + location + pollutant + timestamp\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "deduped_df = bronze_df.dropDuplicates([\n",
    "    \"city\",\n",
    "    \"location_name\",\n",
    "    \"pollutant\",\n",
    "    \"measured_at\"\n",
    "])\n",
    "\n",
    "duplicates_removed = bronze_count - deduped_df.count()\n",
    "\n",
    "print(f\"✅ Deduplication complete.\")\n",
    "print(f\"   Records before : {bronze_count}\")\n",
    "print(f\"   Duplicates removed : {duplicates_removed}\")\n",
    "print(f\"   Records after  : {deduped_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aabf9d40-2c56-40d8-b83c-41a577c6cccd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pollutant filter applied.\n   Records dropped (unknown pollutants) : 0\n   Records remaining : 7215\n\n\uD83D\uDCCA Pollutant breakdown:\n+---------+-----+\n|pollutant|count|\n+---------+-----+\n|     pm25| 2152|\n|       o3| 1155|\n|      no2| 1152|\n|     pm10| 1052|\n|      so2|  852|\n|       co|  852|\n+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 2 — FILTER TO TARGET POLLUTANTS ONLY\n",
    "# Drop any readings for pollutants we don't track\n",
    "# ============================================================\n",
    "\n",
    "filtered_df = deduped_df.filter(\n",
    "    col(\"pollutant\").isin(TARGET_POLLUTANTS)\n",
    ")\n",
    "\n",
    "dropped_pollutants = deduped_df.count() - filtered_df.count()\n",
    "\n",
    "print(f\"✅ Pollutant filter applied.\")\n",
    "print(f\"   Records dropped (unknown pollutants) : {dropped_pollutants}\")\n",
    "print(f\"   Records remaining : {filtered_df.count()}\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA Pollutant breakdown:\")\n",
    "filtered_df.groupBy(\"pollutant\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f08f9a-c738-4194-adcf-5d4373d628ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation complete.\n   Valid records   : 7033\n   Invalid records : 182\n\n\uD83D\uDCCA Invalid record breakdown:\n+---------+-----+\n|pollutant|count|\n+---------+-----+\n|     pm25|  102|\n|       co|   70|\n|      no2|    9|\n|       o3|    1|\n+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 3 — VALIDATE READINGS\n",
    "# Flag records as valid/invalid based on business rules:\n",
    "#   - value must not be NULL\n",
    "#   - value must be >= 0 (no negative concentrations)\n",
    "#   - value must be < 10000 (unrealistically high = sensor error)\n",
    "#   - measured_at must not be NULL\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "validated_df = filtered_df.withColumn(\n",
    "    \"is_valid\",\n",
    "    when(\n",
    "        col(\"value\").isNull() |\n",
    "        (col(\"value\") < 0) |\n",
    "        (col(\"value\") >= 10000) |\n",
    "        col(\"measured_at\").isNull(),\n",
    "        False\n",
    "    ).otherwise(True)\n",
    ")\n",
    "\n",
    "valid_count   = validated_df.filter(col(\"is_valid\") == True).count()\n",
    "invalid_count = validated_df.filter(col(\"is_valid\") == False).count()\n",
    "\n",
    "print(f\"✅ Validation complete.\")\n",
    "print(f\"   Valid records   : {valid_count}\")\n",
    "print(f\"   Invalid records : {invalid_count}\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA Invalid record breakdown:\")\n",
    "validated_df.filter(col(\"is_valid\") == False) \\\n",
    "    .groupBy(\"pollutant\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b63332-65d1-4795-b915-d8186776f78f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AQI categories assigned.\n\n\uD83D\uDCCA AQI Category distribution for PM2.5:\n+--------------------+-----+\n|        aqi_category|count|\n+--------------------+-----+\n|            Moderate|  777|\n|                Good|  713|\n|           Unhealthy|  318|\n|Unhealthy (Sensit...|  233|\n|           Hazardous|   81|\n|      Very Unhealthy|   30|\n+--------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 4 — ASSIGN AQI CATEGORIES\n",
    "# Based on US EPA PM2.5 scale.\n",
    "# Only PM2.5 readings get a meaningful AQI category.\n",
    "# All other pollutants get 'N/A'.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def assign_aqi_category(pollutant, value):\n",
    "    \"\"\"\n",
    "    Assigns AQI category based on PM2.5 value.\n",
    "    Returns 'N/A' for non-PM2.5 pollutants.\n",
    "    \"\"\"\n",
    "    if pollutant != \"pm25\" or value is None:\n",
    "        return \"N/A\"\n",
    "\n",
    "    if 0.0 <= value <= 12.0:\n",
    "        return \"Good\"\n",
    "    elif value <= 35.4:\n",
    "        return \"Moderate\"\n",
    "    elif value <= 55.4:\n",
    "        return \"Unhealthy (Sensitive)\"\n",
    "    elif value <= 150.4:\n",
    "        return \"Unhealthy\"\n",
    "    elif value <= 250.4:\n",
    "        return \"Very Unhealthy\"\n",
    "    else:\n",
    "        return \"Hazardous\"\n",
    "\n",
    "# Register as Spark UDF\n",
    "aqi_udf = udf(assign_aqi_category, StringType())\n",
    "\n",
    "categorised_df = validated_df.withColumn(\n",
    "    \"aqi_category\",\n",
    "    aqi_udf(col(\"pollutant\"), col(\"value\"))\n",
    ")\n",
    "\n",
    "print(f\"✅ AQI categories assigned.\")\n",
    "print(f\"\\n\uD83D\uDCCA AQI Category distribution for PM2.5:\")\n",
    "categorised_df.filter(col(\"pollutant\") == \"pm25\") \\\n",
    "    .groupBy(\"aqi_category\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37da8901-71da-4223-88e9-b900992f7cfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Silver DataFrame ready.\n   Total records   : 7215\n   Valid records   : 7033\n   Invalid records : 182\n\n\uD83D\uDCCA Final Silver Schema:\nroot\n |-- city: string (nullable = true)\n |-- country: string (nullable = true)\n |-- pollutant: string (nullable = true)\n |-- value: double (nullable = true)\n |-- unit: string (nullable = true)\n |-- location_name: string (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- measured_at: timestamp (nullable = true)\n |-- ingested_at: timestamp (nullable = true)\n |-- aqi_category: string (nullable = true)\n |-- is_valid: boolean (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 5 — SELECT FINAL SILVER COLUMNS\n",
    "# Drop raw_json and source_url — not needed beyond Bronze\n",
    "# Keep only what Silver consumers need\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "silver_df = categorised_df.select(\n",
    "    col(\"city\"),\n",
    "    col(\"country\"),\n",
    "    col(\"pollutant\"),\n",
    "    col(\"value\"),\n",
    "    col(\"unit\"),\n",
    "    col(\"location_name\"),\n",
    "    col(\"latitude\"),\n",
    "    col(\"longitude\"),\n",
    "    col(\"measured_at\"),\n",
    "    col(\"ingested_at\"),\n",
    "    col(\"aqi_category\"),\n",
    "    col(\"is_valid\")\n",
    ")\n",
    "\n",
    "print(f\"✅ Silver DataFrame ready.\")\n",
    "print(f\"   Total records   : {silver_df.count()}\")\n",
    "print(f\"   Valid records   : {silver_df.filter(col('is_valid') == True).count()}\")\n",
    "print(f\"   Invalid records : {silver_df.filter(col('is_valid') == False).count()}\")\n",
    "print(f\"\\n\uD83D\uDCCA Final Silver Schema:\")\n",
    "silver_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "902d1fd4-5d23-4f3a-81e2-fff292f1cac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Silver table written successfully.\n   Total records in Silver : 7215\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# WRITE TO SILVER DELTA TABLE\n",
    "# Overwrite ensures Silver always reflects latest clean state\n",
    "# ============================================================\n",
    "\n",
    "(\n",
    "    silver_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    ")\n",
    "\n",
    "# Confirm\n",
    "silver_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {SILVER_TABLE}\").collect()[0][\"cnt\"]\n",
    "\n",
    "print(f\"✅ Silver table written successfully.\")\n",
    "print(f\"   Total records in Silver : {silver_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b5351d-9af1-40c0-9274-472ff7951f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n  \uD83E\uDD48 SILVER TRANSFORMATION — SUMMARY REPORT\n=======================================================\n  Bronze records ingested   : 7215\n  Duplicates removed        : 0\n  Unknown pollutants dropped: 0\n  Valid records             : 7033\n  Invalid records           : 182\n  Final Silver records      : 7215\n=======================================================\n\n\uD83D\uDCCA Average PM2.5 by City (valid only):\n\n+-----------+--------+--------+\n|city       |avg_pm25|readings|\n+-----------+--------+--------+\n|Delhi      |267.44  |50      |\n|Dhaka      |121.64  |100     |\n|Lahore     |120.05  |150     |\n|Cairo      |63.46   |50      |\n|Karachi    |48.7    |150     |\n|Lagos      |45.42   |150     |\n|Mumbai     |36.14   |194     |\n|Mexico City|29.98   |52      |\n|Shanghai   |28.64   |150     |\n|Jakarta    |25.5    |54      |\n|Seoul      |25.08   |150     |\n|Lima       |23.12   |150     |\n|Beijing    |15.58   |100     |\n|Nairobi    |10.83   |150     |\n|Tokyo      |7.55    |150     |\n|New York   |7.09    |100     |\n|London     |6.76    |150     |\n+-----------+--------+--------+\n\n\n▶️  Next Step: Open and run  03_gold_aggregation\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SILVER VALIDATION REPORT\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import round as spark_round, avg\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"  \uD83E\uDD48 SILVER TRANSFORMATION — SUMMARY REPORT\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"  Bronze records ingested   : {bronze_count}\")\n",
    "print(f\"  Duplicates removed        : {duplicates_removed}\")\n",
    "print(f\"  Unknown pollutants dropped: {dropped_pollutants}\")\n",
    "print(f\"  Valid records             : {valid_count}\")\n",
    "print(f\"  Invalid records           : {invalid_count}\")\n",
    "print(f\"  Final Silver records      : {silver_count}\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA Average PM2.5 by City (valid only):\\n\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        city,\n",
    "        ROUND(AVG(value), 2)  AS avg_pm25,\n",
    "        COUNT(*)              AS readings\n",
    "    FROM {SILVER_TABLE}\n",
    "    WHERE pollutant = 'pm25'\n",
    "    AND   is_valid  = true\n",
    "    GROUP BY city\n",
    "    ORDER BY avg_pm25 DESC\n",
    "\"\"\").show(25, truncate=False)\n",
    "\n",
    "print(f\"\\n▶️  Next Step: Open and run  03_gold_aggregation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9ae7168-402a-4d74-904f-9ba7c798e71c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+--------+-----------+\n|city       |aqi_category         |readings|pct_of_city|\n+-----------+---------------------+--------+-----------+\n|Beijing    |Good                 |49      |49.0       |\n|Beijing    |Moderate             |48      |48.0       |\n|Beijing    |Unhealthy (Sensitive)|2       |2.0        |\n|Beijing    |Unhealthy            |1       |1.0        |\n|Cairo      |Unhealthy            |24      |48.0       |\n|Cairo      |Moderate             |14      |28.0       |\n|Cairo      |Unhealthy (Sensitive)|12      |24.0       |\n|Delhi      |Hazardous            |30      |60.0       |\n|Delhi      |Very Unhealthy       |12      |24.0       |\n|Delhi      |Unhealthy            |8       |16.0       |\n|Dhaka      |Unhealthy            |78      |78.0       |\n|Dhaka      |Very Unhealthy       |10      |10.0       |\n|Dhaka      |Hazardous            |8       |8.0        |\n|Dhaka      |Unhealthy (Sensitive)|4       |4.0        |\n|Jakarta    |Moderate             |24      |44.4       |\n|Jakarta    |Unhealthy (Sensitive)|18      |33.3       |\n|Jakarta    |Good                 |12      |22.2       |\n|Karachi    |Moderate             |57      |38.0       |\n|Karachi    |Unhealthy            |54      |36.0       |\n|Karachi    |Unhealthy (Sensitive)|39      |26.0       |\n|Lagos      |Moderate             |99      |66.0       |\n|Lagos      |Unhealthy (Sensitive)|23      |15.3       |\n|Lagos      |Unhealthy            |21      |14.0       |\n|Lagos      |Hazardous            |4       |2.7        |\n|Lagos      |Very Unhealthy       |2       |1.3        |\n|Lagos      |Good                 |1       |0.7        |\n|Lahore     |Hazardous            |39      |26.0       |\n|Lahore     |Moderate             |39      |26.0       |\n|Lahore     |Unhealthy (Sensitive)|32      |21.3       |\n|Lahore     |Unhealthy            |23      |15.3       |\n|Lahore     |Good                 |11      |7.3        |\n|Lahore     |Very Unhealthy       |6       |4.0        |\n|Lima       |Moderate             |106     |70.7       |\n|Lima       |Good                 |21      |14.0       |\n|Lima       |Unhealthy (Sensitive)|19      |12.7       |\n|Lima       |Unhealthy            |4       |2.7        |\n|London     |Good                 |150     |100.0      |\n|Mexico City|Moderate             |29      |55.8       |\n|Mexico City|Good                 |9       |17.3       |\n|Mexico City|Unhealthy            |9       |17.3       |\n|Mexico City|Unhealthy (Sensitive)|5       |9.6        |\n|Mumbai     |Moderate             |61      |31.4       |\n|Mumbai     |Unhealthy            |59      |30.4       |\n|Mumbai     |Good                 |50      |25.8       |\n|Mumbai     |Unhealthy (Sensitive)|24      |12.4       |\n|Nairobi    |Good                 |95      |63.3       |\n|Nairobi    |Moderate             |53      |35.3       |\n|Nairobi    |Unhealthy (Sensitive)|2       |1.3        |\n|New York   |Good                 |91      |91.0       |\n|New York   |Moderate             |9       |9.0        |\n+-----------+---------------------+--------+-----------+\nonly showing top 50 rows\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        city,\n",
    "        aqi_category,\n",
    "        COUNT(*)                                    AS readings,\n",
    "        ROUND(COUNT(*) * 100.0 /\n",
    "            SUM(COUNT(*)) OVER (PARTITION BY city), 1) AS pct_of_city\n",
    "    FROM air_quality_db.silver_clean_measurements\n",
    "    WHERE pollutant  = 'pm25'\n",
    "    AND   is_valid   = true\n",
    "    AND   aqi_category != 'N/A'\n",
    "    GROUP BY city, aqi_category\n",
    "    ORDER BY city, readings DESC\n",
    "\"\"\").show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbfb43b9-2ff1-440c-8f9f-88290332f5dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}